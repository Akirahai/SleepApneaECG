{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import glob\n",
    "import os\n",
    "import wfdb\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import os, tqdm\n",
    "from sklearn.metrics import *\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Conv1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.models import Input, Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./data/SA-2.0.0/\"\n",
    "\n",
    "ir = 3 # interpolate interval\n",
    "before = 2\n",
    "after = 2\n",
    "\n",
    "# normalize\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "\n",
    "    with open(os.path.join(base_dir, \"apnea-ecg.pkl\"), 'rb') as f: # read preprocessing result\n",
    "        apnea_ecg = pickle.load(f)\n",
    "\n",
    "    x_train = []\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_train[i]\n",
    "\t\t# Curve interpolation\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1) \n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
    "        x_train.append([rri_interp_signal, ampl_interp_signal])\n",
    "    x_train = np.array(x_train, dtype=\"float32\").transpose((0, 2, 1)) # convert to numpy format\n",
    "    y_train = np.array(y_train, dtype=\"float32\")\n",
    "\n",
    "    x_test = []\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_test[i]\n",
    "\t\t# Curve interpolation\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
    "        x_test.append([rri_interp_signal, ampl_interp_signal])\n",
    "    x_test = np.array(x_test, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_train, y_train, groups_train, x_test, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelzoo as zoo\n",
    "model = zoo.se_resnext50().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, groups_train, x_test, y_test, groups_test = load_data()\n",
    "class Sleep_Agnea_DB(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data=data\n",
    "        self.label=label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data= self.data[idx].transpose()\n",
    "        label=int(self.label[idx])\n",
    "        return data, label\n",
    "batch_size=128\n",
    "train_data=Sleep_Agnea_DB(x_train,y_train)\n",
    "valid_data=Sleep_Agnea_DB(x_test,y_test)\n",
    "train_loader=DataLoader(train_data,batch_size=batch_size)\n",
    "valid_loader=DataLoader(valid_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs, epoch_stop_decay = 50, 20\n",
    "ckp_path=\"/home/vishc2/tuna/ecg/ecg_ver2/data\"\n",
    "\n",
    "history = {\n",
    "        \"train\": {\"loss\": [], \"f1\": []}, \n",
    "        \"val\": {\"loss\": [], \"f1\": []}, \n",
    "        \"lr\": []\n",
    "    }\n",
    "\n",
    "best_f1 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay= 5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0.1*lr, T_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1/50\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 64/131 [00:07<00:07,  9.23it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"epoch {:2}/{:2}\".format(epoch, epochs) + \"\\n\" + \"-\"*16)\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_labels, running_preds = [], []\n",
    "    for ecgs, labels in tqdm.tqdm(train_loader):\n",
    "        \n",
    "        ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(ecgs.float())\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()*ecgs.size(0)\n",
    "        labels, preds = list(labels.data.cpu().numpy()), list(torch.max(logits, 1)[1].detach().cpu().numpy())\n",
    "        running_labels.extend(labels), running_preds.extend(preds)\n",
    "        \n",
    "        # print(np.argmax(test_labels, axis=1))\n",
    "\n",
    "    epoch_loss, epoch_f1 = running_loss/len(train_loader.dataset), f1_score(running_labels, running_preds, average=\"macro\")\n",
    "    \n",
    "    \n",
    "    history[\"train\"][\"loss\"].append(epoch_loss), history[\"train\"][\"f1\"].append(epoch_f1)\n",
    "    print(\"{:<5} - loss: {:.4f} - f1: {:.4f}\".format(\"train\", epoch_loss, epoch_f1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_labels, running_preds = [], []\n",
    "        for ecgs, labels in tqdm.tqdm(valid_loader):\n",
    "            ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(ecgs.float())\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            running_loss += loss.item()*ecgs.size(0)\n",
    "            labels, preds = list(labels.data.cpu().numpy()), list(torch.max(logits, 1)[1].detach().cpu().numpy())\n",
    "            running_labels.extend(labels), running_preds.extend(preds)\n",
    "\n",
    "    epoch_loss, epoch_f1 = running_loss/len(valid_loader.dataset), f1_score(running_labels, running_preds, average=\"macro\")\n",
    "    \n",
    "    history[\"val\"][\"loss\"].append(epoch_loss), history[\"val\"][\"f1\"].append(epoch_f1)\n",
    "    print(\"{:<5} - loss: {:.4f} - f1: {:.4f}\".format(\"val\", epoch_loss, epoch_f1))\n",
    "\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    if epoch <= epoch_stop_decay:\n",
    "        scheduler.step()\n",
    "\n",
    "    if epoch_f1 > best_f1:\n",
    "        best_f1 = epoch_f1\n",
    "        torch.save(model.state_dict(), \"{}/best_1.pt\".format(ckp_path))\n",
    "\n",
    "with open(\"{}/history_best-1.json\".format(ckp_path), \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "print(\"\\nEvaluation ...\")\n",
    "print(\"=\"*64)\n",
    "\n",
    "model.load_state_dict(torch.load(\"{}/best_1.pt\".format(ckp_path)))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_labels, running_preds = [], []\n",
    "    for ecgs, labels in tqdm.tqdm(valid_loader):\n",
    "        ecgs, labels = ecgs.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(ecgs.float())\n",
    "\n",
    "        labels, preds = list(labels.data.cpu().numpy()), list(torch.max(logits, 1)[1].detach().cpu().numpy())\n",
    "        running_labels.extend(labels), running_preds.extend(preds)\n",
    "\n",
    "print(\"confusion-matrix:\")\n",
    "print(confusion_matrix(\n",
    "    running_labels, running_preds\n",
    "))\n",
    "print(\"classification-report:\")\n",
    "print(classification_report(\n",
    "    running_labels, running_preds\n",
    "    , digits=3\n",
    "))\n",
    "\n",
    "print(\"\\nFinish !!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tuna')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33ce83e0f4f3caa9e775fec3162fcdb84c3202c6a963f2e3ae3ddea94ccd8aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
